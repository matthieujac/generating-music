{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generating Music\n",
    "MGSC-695 Project\n",
    "\n",
    "We are using beethoven music converted in MIDI files to generate music.\n",
    "We have adapted the code from here [link](https://www.analyticsvidhya.com/blog/2020/01/how-to-perform-automatic-music-generation) to fit with our desired music creation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df9dca2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Libraries to be used\n",
    "import random\n",
    "\n",
    "from music21 import *  # understanding music\n",
    "import os  # for listing down the file names\n",
    "import numpy as np  # array processing\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt  # Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading the MIDI Files\n",
    "Understanding the MIDI format: [midi](https://www.cs.cmu.edu/~music/cmsip/readings/MIDI%20tutorial%20for%20programmers.html)\n",
    "- Convert the MIDI file into an array of notes\n",
    "- To simplify the training.  Only the piano channel is selected."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97701965",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# defining function to read MIDI files\n",
    "def read_midi(file):\n",
    "    \n",
    "    print(\"Loading Music File:\",file)\n",
    "    \n",
    "    notes=[]\n",
    "    notes_to_parse = None\n",
    "    \n",
    "    # Parsing a midi file\n",
    "    midi = converter.parse(file)\n",
    "  \n",
    "    # Grouping based on different instruments\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "\n",
    "    # Looping over all the instruments\n",
    "    for part in s2.parts:\n",
    "    \n",
    "        #select elements of only piano\n",
    "        if 'Piano' in str(part): \n",
    "        \n",
    "            notes_to_parse = part.recurse() \n",
    "      \n",
    "            #finding whether a particular element is note or a chord\n",
    "            for element in notes_to_parse:\n",
    "                #note\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                #chord\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return np.array(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform Beethoven Midi files to numpy array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b5812d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: Beethoven/beethoven_opus10_1.mid\n",
      "Loading Music File: Beethoven/beethoven_opus10_2.mid\n",
      "Loading Music File: Beethoven/beethoven_opus10_3.mid\n",
      "Loading Music File: Beethoven/beethoven_opus22_1.mid\n",
      "Loading Music File: Beethoven/beethoven_opus22_2.mid\n",
      "Loading Music File: Beethoven/beethoven_opus22_3.mid\n",
      "Loading Music File: Beethoven/beethoven_opus22_4.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAMUEL~1\\AppData\\Local\\Temp/ipykernel_25940/3579340400.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  notes_array = np.array([read_midi(path+i) for i in files])\n"
     ]
    }
   ],
   "source": [
    "#specify the path\n",
    "path='Beethoven/'\n",
    "\n",
    "#read all the filenames\n",
    "files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "\n",
    "#reading each midi file\n",
    "notes_array = np.array([read_midi(path+i) for i in files])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Appending all the notes together to create a 1D array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7209291",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n"
     ]
    }
   ],
   "source": [
    "#converting 2D array into 1D array\n",
    "notes_ = [element for note_ in notes_array for element in note_]\n",
    "\n",
    "#No. of unique notes\n",
    "unique_notes = list(set(notes_))\n",
    "print(len(unique_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyzing Occurence Frequency of Notes from the Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3715aa2a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 375,
       "height": 248
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#computing frequency of each note\n",
    "freq = dict(Counter(notes_))\n",
    "\n",
    "#consider only the frequencies\n",
    "no=[count for _,count in freq.items()]\n",
    "\n",
    "#plot\n",
    "plt.hist(no)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95ffce20",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
    "print(len(frequent_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the training dataset with 32 timesteps of music notes\n",
    "- This will create a training dataset of size (nb notes x timestep of 32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assign a unique integer to all the notes and chords instead of the music notation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "unique_notes = np.unique(notes_)\n",
    "note_to_int = dict((note_, number) for number, note_ in enumerate(unique_notes))\n",
    "\n",
    "notes_array_int = []\n",
    "for i in notes_array:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append(note_to_int[j])\n",
    "    notes_array_int.append(np.array(temp))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the x and y datasets based on the timestep size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "338432f7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "no_of_timesteps = 32\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for note_ in notes_array_int:\n",
    "    for i in range(0, len(note_) - no_of_timesteps):\n",
    "        \n",
    "        #preparing input and output sequences\n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output)\n",
    "        \n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the training and test set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f154f93",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'moduleTNC' from partially initialized module 'scipy.optimize' (most likely due to a circular import) (C:\\Anaconda\\lib\\site-packages\\scipy\\optimize\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\SAMUEL~1\\AppData\\Local\\Temp/ipykernel_25940/3243094910.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mx_tr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx_val\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_tr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_val\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtest_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.2\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mrandom_state\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     80\u001B[0m     \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_distributor_init\u001B[0m  \u001B[1;31m# noqa: F401\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     81\u001B[0m     \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m__check_build\u001B[0m  \u001B[1;31m# noqa: F401\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 82\u001B[1;33m     \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mbase\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mclone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     83\u001B[0m     \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_show_versions\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mshow_versions\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\base.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m__version__\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0m_config\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mget_config\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_IS_32BIT\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     18\u001B[0m from .utils._tags import (\n\u001B[0;32m     19\u001B[0m     \u001B[0m_DEFAULT_TAGS\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexceptions\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDataConversionWarning\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mdeprecation\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdeprecated\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mfixes\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnp_version\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparse_version\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0m_estimator_html_repr\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mestimator_html_repr\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m from .validation import (\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mscipy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msparse\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0msp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mscipy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mscipy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstats\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mscipy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msparse\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlinalg\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mlsqr\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0msparse_lsqr\u001B[0m  \u001B[1;31m# noqa\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mthreadpoolctl\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\stats\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    439\u001B[0m \"\"\"\n\u001B[0;32m    440\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 441\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mstats\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    442\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mdistributions\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    443\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mmorestats\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\stats\\stats.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mscipy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mspecial\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mspecial\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mscipy\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mlinalg\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 43\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdistributions\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     44\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmstats_basic\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     45\u001B[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\stats\\distributions.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;31m#       instead of `git blame -Lxxx,+x`.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;31m#\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0m_distn_infrastructure\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mrv_discrete\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrv_continuous\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrv_frozen\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_continuous_distns\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;31m# for root finding for continuous distribution ppf, and max likelihood\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;31m# estimation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 24\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mscipy\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0moptimize\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     25\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[1;31m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\optimize\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    399\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    400\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0moptimize\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 401\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0m_minimize\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    402\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0m_root\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    403\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0m_root_scalar\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[1;31m# constrained minimization\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mlbfgsb\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_minimize_lbfgsb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 31\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mtnc\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_minimize_tnc\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     32\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mcobyla\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_minimize_cobyla\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mslsqp\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_minimize_slsqp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\optimize\\tnc.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     33\u001B[0m \"\"\"\n\u001B[0;32m     34\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 35\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mscipy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimize\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmoduleTNC\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     36\u001B[0m from .optimize import (MemoizeJac, OptimizeResult, _check_unknown_options,\n\u001B[0;32m     37\u001B[0m                        _prepare_scalar_function)\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'moduleTNC' from partially initialized module 'scipy.optimize' (most likely due to a circular import) (C:\\Anaconda\\lib\\site-packages\\scipy\\optimize\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "def lstm():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128,return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a Wavenet Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the research article, we can adapt the wavenet model with our notes/chords data to generate music.  Original paper used for the following images: [link](https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio)\n",
    "\n",
    "\n",
    "\n",
    "We start by building the causal dilated convolutianal layers like the following:\n",
    "\n",
    "![image](dilated_causal_convolutianal_layers.png)\n",
    "\n",
    "The full architecture of the wavenet model is similar to this except the residual and skip connections are not used.:\n",
    "\n",
    "![image](wavenet_architecture.png)\n",
    "\n",
    "The first model developed is essentially just the dilated causal convolutional layer.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "257f5576",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 32, 50)            11250     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 32, 64)            9664      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16, 128)           24704     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 256)            98560     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 225)               57825     \n",
      "=================================================================\n",
      "Total params: 267,795\n",
      "Trainable params: 267,795\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow Keras Libraries\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# ---- MODEL CREATION -----\n",
    "model = Sequential()\n",
    "    \n",
    "# The embedding layer creates a dense vector of size (batch, no_of_timesteps, embedding_size)\n",
    "# Weights of the vector are trained during the training process.\n",
    "# The embedding layer allows to learn relationship between notes and chords and is used normally in the natural language processing domain.\n",
    "embedding_size = 100\n",
    "model.add(Embedding(input_dim=len(unique_notes), output_dim=embedding_size, input_length=no_of_timesteps,trainable=True, embeddings_initializer='uniform'))\n",
    "\n",
    "# Creating the causal dilated convolutional layers\n",
    "# First layer\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='causal', dilation_rate=1, activation='relu', ))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(MaxPool1D(pool_size=2, padding='valid'))\n",
    "\n",
    "# Second layer\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu',dilation_rate=2,padding='causal'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(MaxPool1D(pool_size=2, padding='valid'))\n",
    "\n",
    "# Third layer\n",
    "model.add(Conv1D(filters=256, kernel_size=3, activation='relu',dilation_rate=4,padding='causal'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(MaxPool1D(pool_size=2, padding='valid'))\n",
    "          \n",
    "# Downsample the input by taking hte maximum value\n",
    "model.add(GlobalMaxPool1D())\n",
    "    \n",
    "model.add(Dense(256, activation='relu', ))\n",
    "model.add(Dense(len(unique_notes), activation='softmax'))\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24fbf067",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a callback to retain only the best model\n",
    "mc=ModelCheckpoint('best_model_depth1', monitor='val_loss', mode='min', save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b82ace3b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.6146\n",
      "Epoch 00001: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 3s 28ms/step - loss: 4.6132 - val_loss: 4.5287\n",
      "Epoch 2/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.3961\n",
      "Epoch 00002: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 25ms/step - loss: 4.3928 - val_loss: 4.3450\n",
      "Epoch 3/20\n",
      "92/93 [============================>.] - ETA: 0s - loss: 4.1265\n",
      "Epoch 00003: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 3s 28ms/step - loss: 4.1251 - val_loss: 4.1790\n",
      "Epoch 4/20\n",
      "92/93 [============================>.] - ETA: 0s - loss: 3.8951\n",
      "Epoch 00004: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 3.8965 - val_loss: 3.9989\n",
      "Epoch 5/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.7427\n",
      "Epoch 00005: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 25ms/step - loss: 3.7417 - val_loss: 3.8598\n",
      "Epoch 6/20\n",
      "92/93 [============================>.] - ETA: 0s - loss: 3.6140\n",
      "Epoch 00006: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 3s 27ms/step - loss: 3.6135 - val_loss: 3.7685\n",
      "Epoch 7/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.5079\n",
      "Epoch 00007: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 3s 28ms/step - loss: 3.5076 - val_loss: 3.7583\n",
      "Epoch 8/20\n",
      "93/93 [==============================] - ETA: 0s - loss: 3.4373\n",
      "Epoch 00008: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 3.4373 - val_loss: 3.6493\n",
      "Epoch 9/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.3440\n",
      "Epoch 00009: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 25ms/step - loss: 3.3483 - val_loss: 3.6159\n",
      "Epoch 10/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.2751\n",
      "Epoch 00010: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 3s 27ms/step - loss: 3.2739 - val_loss: 3.5671\n",
      "Epoch 11/20\n",
      "93/93 [==============================] - ETA: 0s - loss: 3.2202\n",
      "Epoch 00011: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 3.2202 - val_loss: 3.5352\n",
      "Epoch 12/20\n",
      "92/93 [============================>.] - ETA: 0s - loss: 3.1433\n",
      "Epoch 00012: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 25ms/step - loss: 3.1423 - val_loss: 3.5190\n",
      "Epoch 13/20\n",
      "92/93 [============================>.] - ETA: 0s - loss: 3.0872\n",
      "Epoch 00013: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 27ms/step - loss: 3.0866 - val_loss: 3.4648\n",
      "Epoch 14/20\n",
      "93/93 [==============================] - ETA: 0s - loss: 3.0331\n",
      "Epoch 00014: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 3.0331 - val_loss: 3.4621\n",
      "Epoch 15/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 2.9615\n",
      "Epoch 00015: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 25ms/step - loss: 2.9623 - val_loss: 3.4066\n",
      "Epoch 16/20\n",
      "93/93 [==============================] - ETA: 0s - loss: 2.9313\n",
      "Epoch 00016: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 2.9313 - val_loss: 3.3976\n",
      "Epoch 17/20\n",
      "92/93 [============================>.] - ETA: 0s - loss: 2.8708\n",
      "Epoch 00017: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 2.8713 - val_loss: 3.3713\n",
      "Epoch 18/20\n",
      "92/93 [============================>.] - ETA: 0s - loss: 2.8174\n",
      "Epoch 00018: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 2.8187 - val_loss: 3.3395\n",
      "Epoch 19/20\n",
      "93/93 [==============================] - ETA: 0s - loss: 2.7719\n",
      "Epoch 00019: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 2.7719 - val_loss: 3.3126\n",
      "Epoch 20/20\n",
      "93/93 [==============================] - ETA: 0s - loss: 2.7191\n",
      "Epoch 00020: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 2.7191 - val_loss: 3.3245\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_tr, y_tr,batch_size=128,epochs=20, validation_data=(x_val,y_val),verbose=1, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Increase depth of convolution network\n",
    "- Other articles show that more depth in the convolution layers and starting with smaller filter size helps to increase the quality of the music generation. See this [article](https://towardsdatascience.com/generating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 32, 100)           22500     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 32, 16)            4816      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16, 32)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 64)             6208      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 4, 128)            24704     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 2, 256)            98560     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 225)               57825     \n",
      "=================================================================\n",
      "Total params: 281,973\n",
      "Trainable params: 281,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# ---- MODEL CREATION -----\n",
    "model2 = Sequential()\n",
    "\n",
    "# The embedding layer creates a dense vector of size (batch, no_of_timesteps, embedding_size)\n",
    "# Weights of the vector are trained during the training process.\n",
    "# The embedding layer allows to learn relationship between notes and chords and is used normally in the natural language processing domain.\n",
    "embedding_size = 100\n",
    "model2.add(Embedding(input_dim=len(unique_notes), output_dim=embedding_size, input_length=no_of_timesteps,trainable=True, embeddings_initializer='uniform'))\n",
    "\n",
    "# Creating the causal dilated convolutional layers\n",
    "# ------ ADDED LAYERS --------\n",
    "# LAYER 0.1\n",
    "model2.add(Conv1D(filters=16, kernel_size=3, padding='causal', dilation_rate=1, activation='relu', ))\n",
    "model2.add(Dropout(rate=0.1))\n",
    "model2.add(MaxPool1D(pool_size=2, padding='valid'))\n",
    "\n",
    "# 0.2\n",
    "model2.add(Conv1D(filters=32, kernel_size=3, padding='causal', dilation_rate=1, activation='relu', ))\n",
    "model2.add(Dropout(rate=0.1))\n",
    "model2.add(MaxPool1D(pool_size=2, padding='valid'))\n",
    "\n",
    "# ------------------------------------\n",
    "# First layer\n",
    "model2.add(Conv1D(filters=64, kernel_size=3, padding='causal', dilation_rate=1, activation='relu', ))\n",
    "model2.add(Dropout(rate=0.2))\n",
    "model2.add(MaxPool1D(pool_size=2, padding='valid'))\n",
    "\n",
    "# Second layer\n",
    "model2.add(Conv1D(filters=128, kernel_size=3, activation='relu',dilation_rate=2,padding='causal'))\n",
    "model2.add(Dropout(rate=0.2))\n",
    "model2.add(MaxPool1D(pool_size=2, padding='valid'))\n",
    "\n",
    "# Third layer\n",
    "model2.add(Conv1D(filters=256, kernel_size=3, activation='relu',dilation_rate=4,padding='causal'))\n",
    "model2.add(Dropout(rate=0.2))\n",
    "model2.add(MaxPool1D(pool_size=2, padding='valid'))\n",
    "\n",
    "# Downsample the input by taking hte maximum value\n",
    "model2.add(GlobalMaxPool1D())\n",
    "\n",
    "model2.add(Dense(256, activation='relu', ))\n",
    "model2.add(Dense(len(unique_notes), activation='softmax'))\n",
    "\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model2.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.6444\n",
      "Epoch 00001: val_loss improved from inf to 4.52399, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 41ms/step - loss: 4.6410 - val_loss: 4.5240\n",
      "Epoch 2/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.4403\n",
      "Epoch 00002: val_loss improved from 4.52399 to 4.52077, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 42ms/step - loss: 4.4390 - val_loss: 4.5208\n",
      "Epoch 3/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.4040\n",
      "Epoch 00003: val_loss did not improve from 4.52077\n",
      "93/93 [==============================] - 2s 22ms/step - loss: 4.4064 - val_loss: 4.5435\n",
      "Epoch 4/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.3563\n",
      "Epoch 00004: val_loss improved from 4.52077 to 4.44465, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 39ms/step - loss: 4.3567 - val_loss: 4.4447\n",
      "Epoch 5/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.2783\n",
      "Epoch 00005: val_loss improved from 4.44465 to 4.35053, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 41ms/step - loss: 4.2765 - val_loss: 4.3505\n",
      "Epoch 6/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.1586\n",
      "Epoch 00006: val_loss improved from 4.35053 to 4.28370, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 39ms/step - loss: 4.1577 - val_loss: 4.2837\n",
      "Epoch 7/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.0287\n",
      "Epoch 00007: val_loss improved from 4.28370 to 4.14210, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 40ms/step - loss: 4.0291 - val_loss: 4.1421\n",
      "Epoch 8/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.9294\n",
      "Epoch 00008: val_loss improved from 4.14210 to 4.10737, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 44ms/step - loss: 3.9263 - val_loss: 4.1074\n",
      "Epoch 9/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.8457\n",
      "Epoch 00009: val_loss improved from 4.10737 to 4.09844, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 40ms/step - loss: 3.8479 - val_loss: 4.0984\n",
      "Epoch 10/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.7752\n",
      "Epoch 00010: val_loss improved from 4.09844 to 3.95447, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 40ms/step - loss: 3.7762 - val_loss: 3.9545\n",
      "Epoch 11/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.7009\n",
      "Epoch 00011: val_loss improved from 3.95447 to 3.94172, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 40ms/step - loss: 3.7024 - val_loss: 3.9417\n",
      "Epoch 12/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.6600\n",
      "Epoch 00012: val_loss improved from 3.94172 to 3.88930, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 39ms/step - loss: 3.6621 - val_loss: 3.8893\n",
      "Epoch 13/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.6035\n",
      "Epoch 00013: val_loss improved from 3.88930 to 3.88607, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 39ms/step - loss: 3.6045 - val_loss: 3.8861\n",
      "Epoch 14/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.5596\n",
      "Epoch 00014: val_loss improved from 3.88607 to 3.81640, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 45ms/step - loss: 3.5615 - val_loss: 3.8164\n",
      "Epoch 15/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.5048\n",
      "Epoch 00015: val_loss improved from 3.81640 to 3.78874, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 39ms/step - loss: 3.5050 - val_loss: 3.7887\n",
      "Epoch 16/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.4733\n",
      "Epoch 00016: val_loss improved from 3.78874 to 3.74362, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 40ms/step - loss: 3.4727 - val_loss: 3.7436\n",
      "Epoch 17/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.4480\n",
      "Epoch 00017: val_loss improved from 3.74362 to 3.69312, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 39ms/step - loss: 3.4470 - val_loss: 3.6931\n",
      "Epoch 18/20\n",
      "93/93 [==============================] - ETA: 0s - loss: 3.4191\n",
      "Epoch 00018: val_loss did not improve from 3.69312\n",
      "93/93 [==============================] - 2s 23ms/step - loss: 3.4191 - val_loss: 3.7156\n",
      "Epoch 19/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.3805\n",
      "Epoch 00019: val_loss did not improve from 3.69312\n",
      "93/93 [==============================] - 2s 22ms/step - loss: 3.3812 - val_loss: 3.7207\n",
      "Epoch 20/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.3585\n",
      "Epoch 00020: val_loss improved from 3.69312 to 3.67377, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 39ms/step - loss: 3.3562 - val_loss: 3.6738\n"
     ]
    }
   ],
   "source": [
    "# Run the model and save the best model\n",
    "mc = ModelCheckpoint('best_model_depth2', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "history = model2.fit(x_tr, y_tr, batch_size=128, epochs=20, validation_data=(x_val, y_val), verbose=1, callbacks=[mc])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM Prediction\n",
    "- Instead of using convolution layers, let us see how it works with LSTM layers\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\SAMUEL~1\\AppData\\Local\\Temp/ipykernel_25940/4236634809.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__version__\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msys\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_sys\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 41\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtools\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmodule_util\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_module_util\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     42\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutil\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlazy_loader\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mLazyLoader\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_LazyLoader\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdistribute\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[1;31m# from tensorflow.python import keras\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_column\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mfeature_column_lib\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfeature_column\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m \u001B[1;31m# from tensorflow.python.layers import layers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodule\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\feature_column\\feature_column_lib.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;31m# pylint: disable=unused-import,line-too-long,wildcard-import,g-bad-import-order\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_column\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_column\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     23\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_column\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_column_v2\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_column\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msequence_feature_column\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    145\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msparse_tensor\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0msparse_tensor_lib\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    146\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtensor_shape\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 147\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbase\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    148\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    149\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcheck_ops\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\layers\\base.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0m__future__\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mprint_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlegacy_tf_layers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbase\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[0mInputSpec\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbase\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mInputSpec\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;31m# See b/110718070#comment18 for more details about this import.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmodels\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minput_layer\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mInput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\models.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbackend\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmetrics\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mmetrics_module\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0moptimizer_v1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mfunctional\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtensor_shape\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 34\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mactivations\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     35\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbackend\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbase_layer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\activations.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbackend\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0madvanced_activations\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgeneric_utils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdeserialize_keras_object\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgeneric_utils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mserialize_keras_object\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;31m# Image preprocessing layers.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreprocessing\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage_preprocessing\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mCenterCrop\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreprocessing\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage_preprocessing\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mRandomCrop\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreprocessing\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage_preprocessing\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mRandomFlip\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\preprocessing\\image_preprocessing.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbase_preprocessing_layer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minput_spec\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mInputSpec\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreprocessing\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mimage\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mimage_preprocessing\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcontrol_flow_util\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\preprocessing\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;31m# TODO(mihaimaruseac): remove the import of keras_preprocessing and injecting\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;31m# once we update to latest version of keras_preprocessing\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mkeras_preprocessing\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbackend\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'keras_preprocessing'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\SAMUEL~1\\AppData\\Local\\Temp/ipykernel_22484/2823163336.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;31m#model3.add(Embedding(input_dim=len(unique_notes), output_dim=embedding_size, input_length=no_of_timesteps,trainable=True, embeddings_initializer='uniform'))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mmodel3\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mInput\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m32\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0munique_notes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m128\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[0mmodel3\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mLSTM\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0munits\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m64\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreturn_sequences\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;31m#model3.add(LSTM(units=64, return_sequences=False))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[0mmodel3\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0munits\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m256\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mactivation\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'relu'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    455\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    456\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 457\u001B[1;33m       \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    458\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    459\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprevious_value\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001B[0m in \u001B[0;36madd\u001B[1;34m(self, layer)\u001B[0m\n\u001B[0;32m    219\u001B[0m       \u001B[1;31m# If the model is being built continuously on top of an input layer:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    220\u001B[0m       \u001B[1;31m# refresh its output.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 221\u001B[1;33m       \u001B[0moutput_tensor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlayer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    222\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    223\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, inputs, initial_state, constants, **kwargs)\u001B[0m\n\u001B[0;32m    661\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    662\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0minitial_state\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mconstants\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 663\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mRNN\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__call__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    664\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    665\u001B[0m     \u001B[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    923\u001B[0m     \u001B[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    924\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0m_in_functional_construction_mode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 925\u001B[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001B[0m\u001B[0;32m    926\u001B[0m                                                 input_list)\n\u001B[0;32m    927\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m_functional_construction_call\u001B[1;34m(self, inputs, args, kwargs, input_list)\u001B[0m\n\u001B[0;32m   1115\u001B[0m           \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1116\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menable_auto_cast_variables\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_compute_dtype_object\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1117\u001B[1;33m               \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcast_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1118\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1119\u001B[0m           \u001B[1;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mOperatorNotAllowedInGraphError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, inputs, mask, training, initial_state)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1107\u001B[0m     \u001B[1;31m# LSTM does not support constants. Ignore it during process.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1108\u001B[1;33m     \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minitial_state\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_process_inputs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minitial_state\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1110\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001B[0m in \u001B[0;36m_process_inputs\u001B[1;34m(self, inputs, initial_state, constants)\u001B[0m\n\u001B[0;32m    860\u001B[0m         \u001B[0minitial_state\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstates\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    861\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0minitial_state\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 862\u001B[1;33m       \u001B[0minitial_state\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_initial_state\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    863\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    864\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minitial_state\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstates\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001B[0m in \u001B[0;36mget_initial_state\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m    643\u001B[0m     \u001B[0mdtype\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    644\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mget_initial_state_fn\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 645\u001B[1;33m       init_state = get_initial_state_fn(\n\u001B[0m\u001B[0;32m    646\u001B[0m           inputs=None, batch_size=batch_size, dtype=dtype)\n\u001B[0;32m    647\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001B[0m in \u001B[0;36mget_initial_state\u001B[1;34m(self, inputs, batch_size, dtype)\u001B[0m\n\u001B[0;32m   2521\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2522\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mget_initial_state\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2523\u001B[1;33m     return list(_generate_zero_filled_state_for_cell(\n\u001B[0m\u001B[0;32m   2524\u001B[0m         self, inputs, batch_size, dtype))\n\u001B[0;32m   2525\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001B[0m in \u001B[0;36m_generate_zero_filled_state_for_cell\u001B[1;34m(cell, inputs, batch_size, dtype)\u001B[0m\n\u001B[0;32m   2966\u001B[0m     \u001B[0mbatch_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2967\u001B[0m     \u001B[0mdtype\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2968\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0m_generate_zero_filled_state\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcell\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2969\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2970\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001B[0m in \u001B[0;36m_generate_zero_filled_state\u001B[1;34m(batch_size_tensor, state_size, dtype)\u001B[0m\n\u001B[0;32m   2982\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2983\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_sequence\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2984\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap_structure\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcreate_zeros\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstate_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2985\u001B[0m   \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2986\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mcreate_zeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001B[0m in \u001B[0;36mmap_structure\u001B[1;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[0;32m    633\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    634\u001B[0m   return pack_sequence_as(\n\u001B[1;32m--> 635\u001B[1;33m       \u001B[0mstructure\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mentries\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    636\u001B[0m       expand_composites=expand_composites)\n\u001B[0;32m    637\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    633\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    634\u001B[0m   return pack_sequence_as(\n\u001B[1;32m--> 635\u001B[1;33m       \u001B[0mstructure\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mentries\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    636\u001B[0m       expand_composites=expand_composites)\n\u001B[0;32m    637\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001B[0m in \u001B[0;36mcreate_zeros\u001B[1;34m(unnested_state_size)\u001B[0m\n\u001B[0;32m   2979\u001B[0m     \u001B[0mflat_dims\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtensor_shape\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_shape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0munnested_state_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2980\u001B[0m     \u001B[0minit_state_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mbatch_size_tensor\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mflat_dims\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2981\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minit_state_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2982\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2983\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_sequence\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    199\u001B[0m     \u001B[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    200\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 201\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    202\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    203\u001B[0m       \u001B[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001B[0m in \u001B[0;36mwrapped\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   2745\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2746\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2747\u001B[1;33m     \u001B[0mtensor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2748\u001B[0m     \u001B[0mtensor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_is_zeros_tensor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2749\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mtensor\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001B[0m in \u001B[0;36mzeros\u001B[1;34m(shape, dtype, name)\u001B[0m\n\u001B[0;32m   2792\u001B[0m           \u001B[1;31m# Create a constant if it won't be very big. Otherwise create a fill\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2793\u001B[0m           \u001B[1;31m# op to prevent serialized GraphDefs from becoming too large.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2794\u001B[1;33m           \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_constant_if_small\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mzero\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2795\u001B[0m           \u001B[1;32mif\u001B[0m \u001B[0moutput\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2796\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0moutput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001B[0m in \u001B[0;36m_constant_if_small\u001B[1;34m(value, shape, dtype, name)\u001B[0m\n\u001B[0;32m   2730\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_constant_if_small\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2731\u001B[0m   \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2732\u001B[1;33m     \u001B[1;32mif\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m1000\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2733\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0mconstant\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2734\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mprod\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001B[0m in \u001B[0;36mprod\u001B[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[0;32m   3049\u001B[0m     \u001B[1;33m-\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3050\u001B[0m     \u001B[0mnumber_of_dimensions\u001B[0m \u001B[1;33m:\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3051\u001B[1;33m         \u001B[0mThe\u001B[0m \u001B[0mnumber\u001B[0m \u001B[0mof\u001B[0m \u001B[0mdimensions\u001B[0m \u001B[1;32min\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0ma\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m.\u001B[0m  \u001B[0mScalars\u001B[0m \u001B[0mare\u001B[0m \u001B[0mzero\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mdimensional\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3052\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3053\u001B[0m     \u001B[0mSee\u001B[0m \u001B[0mAlso\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001B[0m in \u001B[0;36m_wrapreduction\u001B[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[0;32m     84\u001B[0m             \u001B[1;31m# support a dtype.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     85\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mdtype\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 86\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mreduction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mpasskwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     87\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     88\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mreduction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mpasskwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36m__array__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    843\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    844\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m__array__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 845\u001B[1;33m     raise NotImplementedError(\n\u001B[0m\u001B[0;32m    846\u001B[0m         \u001B[1;34m\"Cannot convert a symbolic Tensor ({}) to a numpy array.\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    847\u001B[0m         \u001B[1;34m\" This error may indicate that you're trying to pass a Tensor to\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: Cannot convert a symbolic Tensor (lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported"
     ]
    }
   ],
   "source": [
    "# Simple LSTM model\n",
    "K.clear_session()\n",
    "\n",
    "# ---- MODEL CREATION -----\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(input_dim=len(unique_notes), output_dim=embedding_size, input_length=no_of_timesteps,trainable=True, embeddings_initializer='uniform'))\n",
    "model3.add(LSTM(units=128, return_sequences=False))\n",
    "model3.add(Dense(units=256, activation='relu'))\n",
    "model3.add(Dense(units=len(unique_notes), activation='softmax'))\n",
    "model3.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model3.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:176 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 32]\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\SAMUEL~1\\AppData\\Local\\Temp/ipykernel_22484/3071255326.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Run the model and save the best model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mmc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mModelCheckpoint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'best_model_depth2'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmonitor\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'val_loss'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'min'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msave_best_only\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mhistory\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel3\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_tr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_tr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m128\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m20\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_val\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_val\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmc\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    106\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_method_wrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_in_multi_worker_mode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m     \u001B[1;31m# Running inside `run_distribute_coordinator` already.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1096\u001B[0m                 batch_size=batch_size):\n\u001B[0;32m   1097\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1098\u001B[1;33m               \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1099\u001B[0m               \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1100\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    778\u001B[0m       \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    779\u001B[0m         \u001B[0mcompiler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"nonXla\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 780\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    781\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    782\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    821\u001B[0m       \u001B[1;31m# This is the first call of __call__, so we have to initialize.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    822\u001B[0m       \u001B[0minitializers\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 823\u001B[1;33m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_initialize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0madd_initializers_to\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minitializers\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    824\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    825\u001B[0m       \u001B[1;31m# At this point we know that the initialization is complete (or less\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_initialize\u001B[1;34m(self, args, kwds, add_initializers_to)\u001B[0m\n\u001B[0;32m    694\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_graph_deleter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mFunctionDeleter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lifted_initializer_graph\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    695\u001B[0m     self._concrete_stateful_fn = (\n\u001B[1;32m--> 696\u001B[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001B[0m\u001B[0;32m    697\u001B[0m             *args, **kwds))\n\u001B[0;32m    698\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_get_concrete_function_internal_garbage_collected\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2853\u001B[0m       \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2854\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2855\u001B[1;33m       \u001B[0mgraph_function\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2856\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2857\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_maybe_define_function\u001B[1;34m(self, args, kwargs)\u001B[0m\n\u001B[0;32m   3211\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3212\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmissed\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcall_context_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3213\u001B[1;33m       \u001B[0mgraph_function\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_create_graph_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3214\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprimary\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcache_key\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3215\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_create_graph_function\u001B[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001B[0m\n\u001B[0;32m   3063\u001B[0m     \u001B[0marg_names\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbase_arg_names\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mmissing_arg_names\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3064\u001B[0m     graph_function = ConcreteFunction(\n\u001B[1;32m-> 3065\u001B[1;33m         func_graph_module.func_graph_from_py_func(\n\u001B[0m\u001B[0;32m   3066\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3067\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_python_function\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001B[0m in \u001B[0;36mfunc_graph_from_py_func\u001B[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001B[0m\n\u001B[0;32m    984\u001B[0m         \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moriginal_func\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf_decorator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munwrap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpython_func\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    985\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 986\u001B[1;33m       \u001B[0mfunc_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpython_func\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mfunc_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfunc_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    987\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    988\u001B[0m       \u001B[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36mwrapped_fn\u001B[1;34m(*args, **kwds)\u001B[0m\n\u001B[0;32m    598\u001B[0m         \u001B[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    599\u001B[0m         \u001B[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 600\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mweak_wrapped_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__wrapped__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    601\u001B[0m     \u001B[0mweak_wrapped_fn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mweakref\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mref\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwrapped_fn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    602\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    971\u001B[0m           \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint:disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    972\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"ag_error_metadata\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 973\u001B[1;33m               \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mag_error_metadata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    974\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    975\u001B[0m               \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:176 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 32]\n"
     ]
    }
   ],
   "source": [
    "# Run the model and save the best model\n",
    "mc = ModelCheckpoint('best_model_depth2', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "history = model3.fit(x_tr, y_tr, batch_size=128, epochs=20, validation_data=(x_val, y_val), verbose=1, callbacks=[mc])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make predictions using the best model obtained"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the best model to create the upcoming musics.  (best model for the 3 different approaches shown)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "b577243c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loading\n",
    "from keras.models import load_model\n",
    "model = load_model('best_model_depth2')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make a music, we use a random segment of a music from the validation dataset and then the model will predict the next notes over and over again."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple prediction\n",
    "- Using a segment of size 32, we predict the next 32 notes/chords of the music by using the highest predicted note"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "412df988",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115 213 187 115 213 187 115 203 187 115 193 187 111 188 187 111 178 187\n",
      " 111 193 187 111 188 186  97 188 187  97 178 187 111 193]\n",
      "[187, 111, 193, 187, 111, 193, 187, 111, 178, 187, 111, 188, 187, 111, 188, 187, 111, 193, 187, 111, 193, 187, 111, 189, 187, 111, 193, 188, 111, 189, 188, 111]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "ind = np.random.randint(0,len(x_val)-1)\n",
    "initial_music = x_val[ind]  # will not change in the code\n",
    "random_music = x_val[ind]  # will change during the creation of the music (first notes removed)\n",
    "\n",
    "predictions=[]\n",
    "for i in range(32):  # we will predict the second half of the song.\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    y_pred= np.argmax(prob,axis=0)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "\n",
    "print(initial_music)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform back into notes:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [],
   "source": [
    "int_to_note = dict((number, note_) for number, note_ in enumerate(unique_notes))\n",
    "def create_music(note_int):\n",
    "    notes = [int_to_note[i] for i in note_int]\n",
    "    return notes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "88712601",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the predicted notes with the function\n",
    "predicted_notes = create_music(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert notes into MIDI file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2f1fb29a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_midi(prediction_output, name):\n",
    "   \n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        \n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                \n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        # pattern is a note\n",
    "        else:\n",
    "            \n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 1\n",
    "    midi_stream = stream.Stream(output_notes, tempo=120)\n",
    "    midi_stream.write('midi', fp=name+'.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3e38af04",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "convert_to_midi(predicted_notes, name='base_32_2')\n",
    "convert_to_midi(create_music(initial_music)+predicted_notes, name=\"base_64_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make a random decision out of the best notes\n",
    "- Similar to in Reinforcement Learning, we want to perform some exploration to allow for more original creation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115 213 187 115 213 187 115 203 187 115 193 187 111 188 187 111 178 187\n",
      " 111 193 187 111 188 186  97 188 187  97 178 187 111 193]\n",
      "[187, 111, 193, 187, 111, 193, 187, 194, 178, 187, 111, 188, 187, 16, 188, 187, 193, 218, 187, 111, 178, 187, 72, 178, 187, 111, 14, 178, 111, 188, 211, 111]\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters:\n",
    "epsilon = 0.2\n",
    "top_n = 20\n",
    "\n",
    "random_music = initial_music\n",
    "predictions=[]\n",
    "for i in range(32):  # we will predict the second half of the song.\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    # Check the probabilities\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    prob_n = np.argsort(prob)[-top_n:]  # top n notes\n",
    "\n",
    "    # Select the note\n",
    "    rand = np.random.random()\n",
    "    if rand>epsilon:\n",
    "        # Pick best note\n",
    "        y_pred= np.argmax(prob,axis=0)\n",
    "    else:\n",
    "        # Pick random note\n",
    "        y_pred = prob_n[np.random.randint(0, len(prob_n))]\n",
    "\n",
    "    # Prediction\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    # Change the random music size and length with the new note\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "\n",
    "print(initial_music)\n",
    "print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [],
   "source": [
    "predicted_notes_e = create_music(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "# Export\n",
    "convert_to_midi(predicted_notes_e, name='epsilon_32_2')\n",
    "convert_to_midi(create_music(initial_music)+predicted_notes_e, name=\"epsilon_64_2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Beam Search\n",
    "- We'll look two notes away and see what is the highest prediction and based on that, select the current note."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115 213 187 115 213 187 115 203 187 115 193 187 111 188 187 111 178 187\n",
      " 111 193 187 111 188 186  97 188 187  97 178 187 111 193]\n",
      "[187, 111, 193, 187, 97, 193, 187, 111, 178, 187, 111, 188, 187, 111, 188, 187, 111, 193, 187, 111, 193, 187, 111, 189, 187, 111, 193, 188, 111, 189, 188, 111]\n"
     ]
    }
   ],
   "source": [
    "# Perform a beam search with looking two notes ahead.\n",
    "\n",
    "level_1 = 5  # beam search first level limited scope\n",
    "predictions=[]\n",
    "random_music = initial_music\n",
    "for i in range(32):  # we will predict the second half of the song.\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    # Check the probabilities\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    prob_n_notes = np.argsort(prob)[-level_1:]  # the notes with the highest probabilities.\n",
    "    prob_n = np.sort(prob)[-level_1:]  # the probabilities\n",
    "\n",
    "    # Perform the beam search: Check the next predictions to check what is the highest probability path.\n",
    "    max_prob_path = {}  # key is the following y_pred and value is the probability\n",
    "    for prob, note_ in zip(prob_n, prob_n_notes):\n",
    "        # Create the new temporary music path\n",
    "        random_music_temp = np.insert(random_music[0], len(random_music[0]), note_)\n",
    "        random_music_temp = random_music_temp[1:].reshape((1,-1))\n",
    "\n",
    "        # Re-run the prediction after this note selection\n",
    "        prob2 = model.predict(random_music_temp)[0]\n",
    "\n",
    "        # Select the top\n",
    "        prob2_max = np.max(prob2)\n",
    "\n",
    "        # Joint probability\n",
    "        joint = prob * prob2_max\n",
    "\n",
    "        # Append in dictionary\n",
    "        max_prob_path[note_] = joint\n",
    "\n",
    "    # Select the note from the maximum joint path\n",
    "    y_pred = max(max_prob_path, key=max_prob_path.get)\n",
    "\n",
    "    # Actual Prediction\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    # Change the random music size and length with the new note\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "\n",
    "print(initial_music)\n",
    "print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [],
   "source": [
    "predicted_notes_b = create_music(predictions)\n",
    "# Export\n",
    "convert_to_midi(predicted_notes_b, name='beam_32_2')\n",
    "convert_to_midi(create_music(initial_music) + predicted_notes_b, name=\"beam_64_2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Very similar to the initial simple predictions meaning that the first highest prediction is usually the best path! This is pretty intuitive but if the beam search went further it might change the outcome which would be interesting to see."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "If we are a musician trying to discover new music style.  We could use different methods to change the way the music is created.  If we feed our initial composition, then we can explore with different hyper parameters, different suggested music that would help the creation process.  This is why we have shown three different methods to generate music after the model predictions.\n",
    "\n",
    "From the model predictions, we have seen that by adding depth in the convolution layers starting with smaller filter size really helped to have better music which represents beethoven more.  Granted, we are only comparing with 1 starting segment and we are no musical experts...\n",
    "\n",
    "### Limitations\n",
    "In our case, the sampling of the music was done note by note or chord by chord.  Ideally, we would want to develop a sampling on a time basis and have notes or no notes so the tempo would vary.  Also, the midi format has sound intensity which and other parameters which are removed.  Only the notes or chords from the piano channel is extracted so this is a very primitive model compared to other music generator such as MuseNet and Jukebox by OpenAI.\n",
    "\n",
    "### Future idea...\n",
    "Also, the way the creation works is by evaluating the accuracy of the model to predict the next note accurately.  But it would be interesting to analyse a way to generate pleasing music and to evaluate that outcome.  Difficult to say with no music background."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}