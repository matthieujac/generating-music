{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generating Music\n",
    "MGSC-695 Project\n",
    "\n",
    "We are using beethoven music converted in MIDI files to generate music.\n",
    "We have adapted the code from here [link](https://www.analyticsvidhya.com/blog/2020/01/how-to-perform-automatic-music-generation) to fit with our desired music creation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df9dca2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries to be used\n",
    "import random\n",
    "\n",
    "from music21 import *  # understanding music\n",
    "import os  # for listing down the file names\n",
    "import numpy as np  # array processing\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt  # Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading the MIDI Files\n",
    "Understanding the MIDI format: [midi](https://www.cs.cmu.edu/~music/cmsip/readings/MIDI%20tutorial%20for%20programmers.html)\n",
    "- Convert the MIDI file into an array of notes\n",
    "- To simplify the training.  Only the piano channel is selected."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97701965",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# defining function to read MIDI files\n",
    "def read_midi(file):\n",
    "    \n",
    "    print(\"Loading Music File:\",file)\n",
    "    \n",
    "    notes=[]\n",
    "    notes_to_parse = None\n",
    "    \n",
    "    # Parsing a midi file\n",
    "    midi = converter.parse(file)\n",
    "  \n",
    "    # Grouping based on different instruments\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "\n",
    "    # Looping over all the instruments\n",
    "    for part in s2.parts:\n",
    "    \n",
    "        #select elements of only piano\n",
    "        if 'Piano' in str(part): \n",
    "        \n",
    "            notes_to_parse = part.recurse() \n",
    "      \n",
    "            #finding whether a particular element is note or a chord\n",
    "            for element in notes_to_parse:\n",
    "                #note\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                #chord\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return np.array(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform Beethoven Midi files to numpy array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b5812d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: Beethoven/beethoven_opus10_1.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\generating-music\\lib\\site-packages\\music21\\midi\\translate.py:883: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent SEQUENCE_TRACK_NAME, track=5, channel=None, data=b'Copyright \\xa9 2008 by Bernd Krueger'>; getting generic Instrument\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: Beethoven/beethoven_opus10_2.mid\n",
      "Loading Music File: Beethoven/beethoven_opus10_3.mid\n",
      "Loading Music File: Beethoven/beethoven_opus22_1.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\generating-music\\lib\\site-packages\\music21\\midi\\translate.py:883: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent SEQUENCE_TRACK_NAME, track=5, channel=None, data=b'Copyright \\xa9 2009 by Bernd Krueger'>; getting generic Instrument\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: Beethoven/beethoven_opus22_2.mid\n",
      "Loading Music File: Beethoven/beethoven_opus22_3.mid\n",
      "Loading Music File: Beethoven/beethoven_opus22_4.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samuel Dion\\AppData\\Local\\Temp\\ipykernel_50044\\3579340400.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  notes_array = np.array([read_midi(path+i) for i in files])\n"
     ]
    }
   ],
   "source": [
    "#specify the path\n",
    "path='Beethoven/'\n",
    "\n",
    "#read all the filenames\n",
    "files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "\n",
    "#reading each midi file\n",
    "notes_array = np.array([read_midi(path+i) for i in files])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Appending all the notes together to create a 1D array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7209291",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n"
     ]
    }
   ],
   "source": [
    "#converting 2D array into 1D array\n",
    "notes_ = [element for note_ in notes_array for element in note_]\n",
    "\n",
    "#No. of unique notes\n",
    "unique_notes = list(set(notes_))\n",
    "print(len(unique_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyzing Occurence Frequency of Notes from the Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3715aa2a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAu4AAAHwCAYAAAD5Keq8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAABYlAAAWJQFJUiTwAAAoZElEQVR4nO3de7i1ZV0v+u8vSFDM10OarmyHtATpaGAHoQRxbReeKXHJ7lLJUtPlIQ9YbkWj0rYt8UxL16aWWLSDFS51YUhZQKC4KkGztiiovBaGFSAvcRb97T/GM7dzzeZ8DzDnHON+38/nusZ1M+7DeO4xH+ac33m/z7if6u4AAACL7VvmPQEAAGDHBHcAABiA4A4AAAMQ3AEAYACCOwAADEBwBwCAAQjuAAAwAMEdAAAGILgDAMAABHcAABiA4A4AAAMQ3AEAYAB7z3sCi6KqrkxyryRb5zwVAAB2b/snuaG7H7IrgwT3b7rX3e9+9/sefPDB9533RAAA2H1ddtllueWWW3Z53LoE96o6NskRSR6e5IeSfFuS3+/uZ2xnzF5Jnp3kWUl+IMm+Sa5O8ldJXtvdl68y5vgkL0zyvUm+nuSTSU7u7g+tw9vYevDBB9/3kksuWYeXAgCA1R166KG59NJLt+7quPVacT8xs8B+Y5Krkjxse52r6p5JPpjkqCSfSvLeJLcm+c4kP5nkwCSXrxhzcpJXTK9/apK7JTkuydlV9eLuPmWd3gsAACyc9QruL8ssUH8+s5X383fQ/79kFtqf393/ZWVjVX3riueHZRbav5DkR7r7q1P9m5JckuTkqvpQd2+9i+8DAAAW0rrsKtPd53f3Fd3dO+pbVYck+ZkkZ64W2qfX+9qKqudP5RuWQvvUb2uS30qyT2aX3QAAwG5pHttB/sxU/kFVbamqZ1TV/1lVz6uqf7vGmKOm8txV2j68og8AAOx25rGrzI9M5XdndunL/Za1dVW9K8lLuvvrSVJV+2V27fuN3X31Kq93xVQeuDMHr6q1Pn263evyAQBgnuax4v6AqXxLkguSHJzZLjT/LrMg/x+TvHZZ/y1TuW2N11uqv/d6ThIAABbJPFbcl/5Y+GySpy+trCf5s2lbyUuTvLyqfqO7b1/vg3f3oavVTyvxh6z38QAAYD3MY8X9+qk8e1loT5J0918nuTKzFfiDp+qlFfUtWd1S/fVrtAMAwPDmEdw/N5XXr9G+tGvM3ZOku29K8uUk96yqB63S/6FT+a9u2AQAALuLeQT3P53K71/ZUFX75JtBfOuypvOm8uhVXu9xK/oAAMBuZx7B/X1J/iHJ06vqR1e0vTazS1/O7+6vLKt/91S+pqrus1RZVfsneWGS25K8Z8NmDAAAc7YuH06tqmOSHDM9feBUPrKqTpv++5ruPiGZXfpSVT+b5ENJLqqq/57ZpTA/luQnkvxTkl9Y/vrdfXFVvSXJy5N8uqrOSnK3JE9Pct8kL3bXVAAAdmfrtavMw5Mcv6LugOmRJF9KcsJSQ3d/ZFptf21m20BuSfKVzFbWf727/2HlAbr7FVX1N5mtsD8vyTcy24HmTd39oXV6HwAAsJDWJbh390lJTtrFMX+d5NhdHHNaktN2ZQwAAOwO5nGNOwAAsIsEdwAAGIDgDgAAAxDcAQBgAOu1qwx3wf6v+qN5T2HTbX3jE+Y9BQCAoVhxBwCAAQjuAAAwAMEdAAAGILgDAMAABHcAABiA4A4AAAMQ3AEAYACCOwAADEBwBwCAAQjuAAAwAMEdAAAGILgDAMAABHcAABiA4A4AAAMQ3AEAYACCOwAADEBwBwCAAQjuAAAwAMEdAAAGILgDAMAABHcAABiA4A4AAAMQ3AEAYACCOwAADEBwBwCAAQjuAAAwAMEdAAAGILgDAMAABHcAABiA4A4AAAMQ3AEAYACCOwAADEBwBwCAAQjuAAAwgHUJ7lV1bFW9s6ouqqobqqqr6vRdGP/b05iuqn+7Rp+9quplVfXpqrqlqq6rqnOq6rD1eA8AALDI1mvF/cQkL0ry8CRf3pWBVfWkJD+f5Mbt9KkkZyR5S5K7JTklyfuTPCrJhVX1lDs1awAAGMR6BfeXJTkwyb2SvGBnB1XV/ZOcmuTMJJdsp+txSY5NcnGSh3f3K7v755M8OsnXk5xaVd92J+cOAAALb12Ce3ef391XdHfv4tD/eypfuIN+S38MnNjdty477l9lFvrvn1mwBwCA3dLcPpxaVT+b5Jgkv9Dd126n375JDktyc5KLVuny4ak8ap2nCAAAC2PveRy0qr47yduTnN7dH9xB9+9JsleSL3b3Hau0XzGVB+7ksde6JOdhOzMeAADmYdNX3KvqW5K8N7MPo75kJ4Zsmcpta7Qv1d/7rs0MAAAW1zxW3F+W5IgkT+jur272wbv70NXqp5X4QzZ5OgAAsFM2dcW9qg5M8oYk7+nuc3Zy2NKK+pY12pfqr78LUwMAgIW22ZfKfG+SfZI8e9kNl7qqOrNV+CS5Yqo7Znr+hcy2fDygqlb7F4KHTuXlGzlxAACYp82+VGZrkt9Zo+0JSR6Y5A+T3DD1TXffWlUXJ/nJ6XH+inGPm8rz1nmuAACwMDY1uHf3p5I8Z7W2qrogs+D+6u7+/Irmd2UW2l9fVY9Z2su9qn4kydOT/HOS923QtAEAYO7WJbhPl7UcMz194FQ+sqpOm/77mu4+4S4c4owkP53ZTZY+WVVnJ7lfZqF9ryTP7e4b7sLrAwDAQluvFfeHJzl+Rd0B0yNJvpTkTgf37u6q+j+SXJzk55K8OMmtSS5M8vruvvjOvjYAAIxgXYJ7d5+U5KS7+BpH7qD9jiRvnR4AALBH2fQbMAEAALtOcAcAgAEI7gAAMADBHQAABiC4AwDAAAR3AAAYgOAOAAADENwBAGAAgjsAAAxAcAcAgAEI7gAAMADBHQAABiC4AwDAAAR3AAAYgOAOAAADENwBAGAAgjsAAAxAcAcAgAEI7gAAMADBHQAABiC4AwDAAAR3AAAYgOAOAAADENwBAGAAgjsAAAxAcAcAgAEI7gAAMADBHQAABiC4AwDAAAR3AAAYgOAOAAADENwBAGAAgjsAAAxAcAcAgAEI7gAAMADBHQAABiC4AwDAAAR3AAAYwLoE96o6tqreWVUXVdUNVdVVdfoafR9aVb9cVedV1d9X1e1V9Y9V9cGqevQOjnN8Vf1lVd1YVduq6oKqeuJ6vAcAAFhk67XifmKSFyV5eJIv76Dvryd5Y5LvSHJOkjcn+ViSJyQ5r6pestqgqjo5yWlJHpTk1CSnJ/mBJGdX1Yvu8jsAAIAFtvc6vc7LklyV5PNJjkhy/nb6npvkN7v7k8srq+qIJB9J8qaq+sPuvnpZ22FJXpHkC0l+pLu/OtW/KcklSU6uqg9199Z1ej8AALBQ1mXFvbvP7+4rurt3ou9pK0P7VP/nSS5Icrckh61ofv5UvmEptE9jtib5rST7JHn2nZs9AAAsvkX7cOrXpvKOFfVHTeW5q4z58Io+AACw21mvS2Xusqr67iSPSXJzkguX1e+X5DuT3Lj88pllrpjKA3fyOJes0fSwnZ8tAABsroUI7lW1T5Lfz+ySl19afjlMki1TuW2N4Uv1996Y2QEAwPzNPbhX1V5Jfi/J4UnOTHLyRh6vuw9dYx6XJDlkI48NAAB31lyvcZ9C++lJnpbkvyV5xiofcF1aUd+S1S3VX7/uEwQAgAUxt+BeVd+a5A+SHJfk/0nyM9298kOp6e6bMtsb/p5V9aBVXuqhU3n5Rs0VAADmbS7BvaruluQPM1tp/90kz+zur29nyHlTefQqbY9b0QcAAHY7mx7cpw+ivj/JU5L8TpJnd/c3djDs3VP5mqq6z7LX2j/JC5PcluQ96z9bAABYDOvy4dSqOibJMdPTB07lI6vqtOm/r+nuE6b/fneSxye5JrNLYF5XVStf8oLuvmDpSXdfXFVvSfLyJJ+uqrMyu1HT05PcN8mL3TUVAIDd2XrtKvPwJMevqDtgeiTJl5IsBfeHTOW3J3nddl7zguVPuvsVVfU3ma2wPy/JN5JcmuRN3f2hOztxAAAYwboE9+4+KclJO9n3yLtwnNOSnHZnxwMAwKjmuh0kAACwcwR3AAAYgOAOAAADENwBAGAAgjsAAAxAcAcAgAEI7gAAMADBHQAABiC4AwDAAAR3AAAYgOAOAAADENwBAGAAgjsAAAxAcAcAgAEI7gAAMADBHQAABiC4AwDAAAR3AAAYgOAOAAADENwBAGAAgjsAAAxAcAcAgAEI7gAAMADBHQAABiC4AwDAAAR3AAAYgOAOAAADENwBAGAAgjsAAAxAcAcAgAEI7gAAMADBHQAABiC4AwDAAAR3AAAYgOAOAAADENwBAGAAgjsAAAxAcAcAgAEI7gAAMIB1Ce5VdWxVvbOqLqqqG6qqq+r0HYw5rKrOqarrquqWqvp0Vb20qvbazpgnVtUFVbWtqm6sqr+oquPX4z0AAMAi23udXufEJD+U5MYkVyV52PY6V9VTkrwvya1JzkxyXZInJXlrksOTPG2VMS9K8s4k1yY5PcntSY5NclpV/UB3n7BO7wUAABbOel0q87IkBya5V5IXbK9jVd0ryalJvp7kyO7++e5+ZZKHJ/l4kmOr6rgVY/ZPcnJmAf8R3f3C7n5Zkh9M8oUkr6iqR67TewEAgIWzLsG9u8/v7iu6u3ei+7FJ7p/kjO7+xLLXuDWzlfvkX4f/n0uyT5JTunvrsjFfTfIb09Pn38npAwDAwluvS2V2xVFTee4qbRcmuTnJYVW1T3ffthNjPryiz3ZV1SVrNG338h4AAJineewqc9BUXr6yobvvSHJlZn9QHLCTY65OclOSB1fVPdZ3qgAAsBjmseK+ZSq3rdG+VH/vXRyz39Tv5u0dvLsPXa1+Wok/ZHtjAQBgXuzjDgAAA5hHcF9aNd+yRvtS/fV3YsxaK/IAADC0eQT3z03lgSsbqmrvJA9JckeSL+7kmAdldpnMVd293ctkAABgVPMI7udN5dGrtD0qyT2SXLxsR5kdjXncij4AALDbmUdwPyvJNUmOq6pHLFVW1b5JXj89fdeKMe9JcluSF003Y1oac58kr56evnujJgwAAPO2LrvKVNUxSY6Znj5wKh9ZVadN/31Nd5+QJN19Q1U9N7MAf0FVnZHZHVGfnNm2j2clOXP563f3lVX1yiTvSPKJqjozye2Z3czpwUne3N0fX4/3AgAAi2i9toN8eJLjV9QdkG/uxf6lJCcsNXT3B6rqiCSvSfLUJPsm+XySlyd5x2p3YO3ud1bV1ul1npXZvxZ8JsmJ3f3edXofAACwkNYluHf3SUlO2sUxH0vy+F0cc3aSs3dlDAAA7A7s4w4AAAMQ3AEAYACCOwAADEBwBwCAAQjuAAAwAMEdAAAGILgDAMAABHcAABiA4A4AAAMQ3AEAYACCOwAADEBwBwCAAQjuAAAwAMEdAAAGILgDAMAABHcAABiA4A4AAAMQ3AEAYACCOwAADEBwBwCAAQjuAAAwAMEdAAAGILgDAMAABHcAABiA4A4AAAMQ3AEAYACCOwAADEBwBwCAAQjuAAAwAMEdAAAGILgDAMAABHcAABiA4A4AAAMQ3AEAYACCOwAADEBwBwCAAQjuAAAwAMEdAAAGMNfgXlVPqKo/qaqrquqWqvpiVf1hVT1yjf6HVdU5VXXd1P/TVfXSqtprs+cOAACbaW7Bvap+M8mHkhyS5Nwkb09yaZKnJPlYVT1jRf+nJLkwyaOSvD/JKUnuluStSc7YvJkDAMDm23seB62qByY5Ick/JvnB7v6nZW2PTnJekl9LcvpUd68kpyb5epIju/sTU/1rp77HVtVx3S3AAwCwW5rXivt3T8f+i+WhPUm6+/wk/5Lk/suqj52en7EU2qe+tyY5cXr6gg2dMQAAzNG8gvsVSW5P8qNV9e3LG6rqUUm+LcmfLqs+airPXeW1Lkxyc5LDqmqfDZgrAADM3Vwulenu66rql5O8JclnquoDSa5N8j1JnpzkI0l+YdmQg6by8lVe646qujLJ9yU5IMll2zt2VV2yRtPDduU9AADAZppLcE+S7n5bVW1N8l+TPHdZ0+eTnLbiEpotU7ltjZdbqr/3es4RAAAWxTx3lfmlJGclOS2zlfb9khya5ItJfr+q/tNGHLe7D13tkeSzG3E8AABYD3MJ7lV1ZJLfTPI/uvvl3f3F7r65uy9N8lNJvpzkFVV1wDRkaUV9y796sf+1/vqNmTEAAMzXvFbcnziV569s6O6bk/xlZnP74an6c1N54Mr+VbV3kockuSOz1XoAANjtzCu4L+3+cv812pfqb5/K86by6FX6PirJPZJc3N23rc/0AABgscwruF80lc+rqu9c3lBVj0tyeJJbk1w8VZ+V5Jokx1XVI5b13TfJ66en79rQGQMAwBzNa1eZszLbp/3fJbmsqt6f5CtJDs7sMppK8qruvjZJuvuGqnruNO6CqjojyXWZbR150FR/5qa/CwAA2CTz2sf9G1X1+CQvTHJcZh9IvUdmYfycJO/o7j9ZMeYDVXVEktckeWqSfTPbOvLlU//exLcAAACbap77uH8tydumx86O+ViSx2/QlAAAYGHNbR93AABg5wnuAAAwAMEdAAAGILgDAMAABHcAABiA4A4AAAMQ3AEAYACCOwAADEBwBwCAAQjuAAAwAMEdAAAGILgDAMAABHcAABiA4A4AAAMQ3AEAYACCOwAADEBwBwCAAQjuAAAwAMEdAAAGILgDAMAABHcAABiA4A4AAAMQ3AEAYACCOwAADEBwBwCAAQjuAAAwAMEdAAAGILgDAMAABHcAABiA4A4AAAMQ3AEAYACCOwAADEBwBwCAAQjuAAAwAMEdAAAGILgDAMAABHcAABiA4A4AAAMQ3AEAYABzD+5V9Ziqen9VfaWqbquqf6iqP66qx6/S97CqOqeqrquqW6rq01X10qraax5zBwCAzbL3PA9eVf8pySuTXJXkfyS5Jsn9kxya5Mgk5yzr+5Qk70tya5Izk1yX5ElJ3prk8CRP28SpAwDApppbcK+q52YW2t+b5HndffuK9m9d9t/3SnJqkq8nObK7PzHVvzbJeUmOrarjuvuMzZo/AABsprlcKlNV+yR5Q5K/yyqhPUm6+2vLnh6b2Ur8GUuhfepza5ITp6cv2LgZAwDAfM1rxf1/zyyIvy3JN6rqCUm+P7PLYP6yuz++ov9RU3nuKq91YZKbkxxWVft0923bO3BVXbJG08N2cu4AALDp5hXcf2Qqb03yycxC+/+vqi5Mcmx3//NUddBUXr7yhbr7jqq6Msn3JTkgyWUbMmMAAJijeQX3B0zlK5N8JslPJvlUkockOTnJY5P8YWYfUE2SLVO5bY3XW6q/944O3N2HrlY/rcQfsqPxAAAwD/PaDnLpuHckeXJ3f7S7b+zuv0nyU5ntMnNEVT1yTvMDAICFMq/gfv1UfrK7ty5v6O6bk/zx9PRHp3JpRX1LVrdUf/0a7QAAMLR5BffPTeX1a7R/dSrvvqL/gSs7VtXemV1ic0eSL67T/AAAYKHMK7j/WZJO8r1Vtdoclj6seuVUnjeVR6/S91FJ7pHk4h3tKAMAAKOaS3Dv7i8lOTvJ/5bkF5e3VdVjk/z7zFbjl7Z/PCuzu6oeV1WPWNZ33ySvn56+a2NnDQAA8zO3O6cmeWGSH07ylmkf909mdsnLMZndIfU53b0tSbr7hulOq2cluaCqzkhyXZInZ7ZV5FlJztz0dwAAAJtkXpfKpLuvSnJoklOSPDSzlfcjM1uJP7y737ei/weSHJHZDZeemuTFSb6W5OVJjuvu3qy5AwDAZpvninumGyy9eHrsTP+PJXn8hk4KAAAW0NxW3AEAgJ0nuAMAwAAEdwAAGIDgDgAAAxDcAQBgAII7AAAMQHAHAIABCO4AADAAwR0AAAYguAMAwAAEdwAAGIDgDgAAAxDcAQBgAII7AAAMQHAHAIABCO4AADAAwR0AAAYguAMAwAAEdwAAGIDgDgAAAxDcAQBgAII7AAAMQHAHAIABCO4AADAAwR0AAAYguAMAwAAEdwAAGIDgDgAAAxDcAQBgAII7AAAMQHAHAIABCO4AADAAwR0AAAYguAMAwAAEdwAAGIDgDgAAAxDcAQBgAII7AAAMYGGCe1U9o6p6ejxnjT5PrKoLqmpbVd1YVX9RVcdv9lwBAGCzLURwr6rvSnJKkhu30+dFSc5O8v1JTk9yapJ/k+S0qjp5M+YJAADzMvfgXlWV5D1Jrk3y7jX67J/k5CTXJXlEd7+wu1+W5AeTfCHJK6rqkZszYwAA2HxzD+5JXpLkqCTPTnLTGn1+Lsk+SU7p7q1Lld391SS/MT19/gbOEQAA5mquwb2qDk7yxiRv7+4Lt9P1qKk8d5W2D6/oAwAAu52953Xgqto7ye8l+bskr95B94Om8vKVDd19dVXdlOTBVXWP7r55B8e9ZI2mh+1gDgAAMDdzC+5JXpfkh5P8RHffsoO+W6Zy2xrt25LsN/XbbnAHAIARzSW4V9WPZbbK/ubu/vhmHru7D11jTpckOWQz5wIAADtr069xny6R+d3MLnt57U4OW1pp37JG+45W5AEAYGjz+HDqPZMcmOTgJLcuu+lSJ/mVqc+pU93bpuefm8oDV75YVT0os8tkrtrR9e0AADCqeVwqc1uS31mj7ZDMrnv/aGZhfekymvOSHJ7k6GV1Sx63rA8AAOyWNj24Tx9Efc5qbVV1UmbB/b3d/dvLmt6T5JeSvKiq3rO0l3tV3Sff3JFm1Zs3AQDA7mCeu8rstO6+sqpemeQdST5RVWcmuT3JsUkenDl8yBUAADbTEME9Sbr7nVW1NckJSZ6V2fX5n0lyYne/d55zAwCAjbZQwb27T0py0nbaz05y9mbNBwAAFsU8dpUBAAB2keAOAAADENwBAGAAgjsAAAxAcAcAgAEI7gAAMADBHQAABiC4AwDAAAR3AAAYgOAOAAADENwBAGAAgjsAAAxAcAcAgAEI7gAAMADBHQAABiC4AwDAAAR3AAAYgOAOAAADENwBAGAAgjsAAAxAcAcAgAEI7gAAMADBHQAABiC4AwDAAAR3AAAYgOAOAAADENwBAGAAgjsAAAxAcAcAgAEI7gAAMADBHQAABiC4AwDAAAR3AAAYgOAOAAADENwBAGAAgjsAAAxg73lPgD3T/q/6o3lPgU2w9Y1PmPcUAGC3YcUdAAAGMJfgXlX3q6rnVNX7q+rzVXVLVW2rqo9W1c9X1arzqqrDquqcqrpuGvPpqnppVe212e8BAAA207wulXlakncluTrJ+Un+Lsl3JPnpJL+d5HFV9bTu7qUBVfWUJO9LcmuSM5Ncl+RJSd6a5PDpNQEAYLc0r+B+eZInJ/mj7v7GUmVVvTrJXyZ5amYh/n1T/b2SnJrk60mO7O5PTPWvTXJekmOr6rjuPmNT3wUAAGySuVwq093ndffZy0P7VP+VJO+enh65rOnYJPdPcsZSaJ/635rkxOnpCzZuxgAAMF+L+OHUr03lHcvqjprKc1fpf2GSm5McVlX7bOTEAABgXhZqO8iq2jvJs6any0P6QVN5+cox3X1HVV2Z5PuSHJDksh0c45I1mh62a7MFAIDNs2gr7m9M8v1JzunuP15Wv2Uqt60xbqn+3hs0LwAAmKuFWXGvqpckeUWSzyZ55kYdp7sPXeP4lyQ5ZKOOCwAAd8VCrLhX1YuSvD3JZ5I8uruvW9FlaUV9S1a3VH/9+s8OAADmb+7BvapemuSdSf42s9D+lVW6fW4qD1xl/N5JHpLZh1m/uEHTBACAuZprcK+qX87sBkqfyiy0/9MaXc+byqNXaXtUknskubi7b1v3SQIAwAKYW3Cfbp70xiSXJHlMd1+zne5nJbkmyXFV9Yhlr7FvktdPT9+1UXMFAIB5m8uHU6vq+CS/ltmdUC9K8pKqWtlta3efliTdfUNVPTezAH9BVZ2R5LrM7r560FR/5ubMHgAANt+8dpV5yFTuleSla/T58ySnLT3p7g9U1RFJXpPkqUn2TfL5JC9P8o7u7o2aLAAAzNtcgnt3n5TkpDsx7mNJHr/e8wEAgEU3911lAACAHRPcAQBgAII7AAAMQHAHAIABCO4AADAAwR0AAAYguAMAwAAEdwAAGIDgDgAAAxDcAQBgAII7AAAMQHAHAIABCO4AADAAwR0AAAYguAMAwAAEdwAAGIDgDgAAAxDcAQBgAII7AAAMQHAHAIABCO4AADAAwR0AAAYguAMAwAAEdwAAGIDgDgAAAxDcAQBgAII7AAAMQHAHAIABCO4AADAAwR0AAAYguAMAwAD2nvcEgN3X/q/6o3lPYdNtfeMT5j2FTec8A2wOK+4AADAAwR0AAAYguAMAwAAEdwAAGIDgDgAAA7CrDMA62hN3WNkTOc+7vz1x56A98f/r0c6zFXcAABjAUCvuVfXgJL+W5Ogk90tydZIPJPnV7v7qHKcGAOxG9sTVZxbfMMG9qr4nycVJHpDkg0k+m+RHk/xikqOr6vDuvnaOUwQAgA0z0qUy/zmz0P6S7j6mu1/V3UcleWuSg5K8Ya6zAwCADTREcJ9W2x+bZGuS31rR/CtJbkryzKrab5OnBgAAm2KI4J7k0VP5J939jeUN3f0vST6W5B5JfnyzJwYAAJthlGvcD5rKy9dovyKzFfkDk/zZ9l6oqi5Zo+mHLrvsshx66KF3boZ3wdVf3rbpxwQA2NMd+pHXzeW4l112WZLsv6vjRgnuW6ZyrYS7VH/vu3CMr99yyy3bLr300q134TV21cOm8rObeEzWj/M3LudubM7fuJy7se125+/Sf5zbofdPcsOuDholuK+b7t78JfU1LK3+L9Kc2HnO37icu7E5f+Ny7sbm/M3fKNe4L62ob1mjfan++o2fCgAAbL5RgvvnpvLANdofOpVrXQMPAABDGyW4nz+Vj62q/2XOVfVtSQ5PcnOS/7nZEwMAgM0wRHDv7i8k+ZPMLuR/4YrmX02yX5Lf6+6bNnlqAACwKUb6cOp/THJxkndU1WOSXJbkxzLb4/3yJK+Z49wAAGBDVXfPew47raq+K8mvJTk6yf2SXJ3k/Ul+tbu/Os+5AQDARhoquAMAwJ5qiGvcAQBgTye4AwDAAAR3AAAYgOAOAAADENwBAGAAgjsAAAxAcJ+DqnpwVf3XqvqHqrqtqrZW1duq6j7zntuepKqOrap3VtVFVXVDVXVVnb6DMYdV1TlVdV1V3VJVn66ql1bVXtsZ88SquqCqtlXVjVX1F1V1/Pq/oz1DVd2vqp5TVe+vqs9P52FbVX20qn6+qlb9uebcLY6q+s2q+rOq+vvpXFxXVZ+sql+pqvutMcb5W1BV9Yzp52dX1XPW6LPL56Kqjq+qv5z6b5vGP3Fj3sWeYcobvcbjK2uM8b23QOzjvsmq6nsyuwPsA5J8MMlnk/xoZneA/VySw7v72vnNcM9RVZ9K8kNJbkxyVZKHJfn97n7GGv2fkuR9SW5NcmaS65I8KclBSc7q7qetMuZFSd6Z5NppzO1Jjk3y4CRv7u4T1vdd7f6q6vlJ3pXZDdjOT/J3Sb4jyU8n2ZLZOXpaL/vh5twtlqq6PcmlST6T5J+S7Jfkx5M8Isk/JPnx7v77Zf2dvwU13Rjxb5LsleSeSZ7b3b+9os8un4uqOjnJKzL72XxWkrslOS7JfZO8uLtP2aj3tDurqq1J7p3kbas039jdJ6/o73tv0XS3xyY+kvxxks7sB8/y+rdM9e+e9xz3lEdmfyw9NEklOXL6+p++Rt97ZRYwbkvyiGX1+2b2h1gnOW7FmP0z+2F3bZL9l9XfJ8nnpzGPnPfXYbRHkqMy+8XxLSvqH5hZiO8kT3XuFveRZN816t8wfW3/s/O3+I/pZ+efJvlCkjdNX9fn3NVzkeSwqf7zSe6z4rWunV5v/416X7vzI8nWJFt3sq/vvQV8uFRmE02r7Y/N7Bvnt1Y0/0qSm5I8s6r22+Sp7ZG6+/zuvqKnnyo7cGyS+yc5o7s/sew1bk1y4vT0BSvG/FySfZKc0t1bl435apLfmJ4+/05Of4/V3ed199nd/Y0V9V9J8u7p6ZHLmpy7BTN97Vfz36byocvqnL/F9ZLM/pB+dma/v1ZzZ87F0vM3TP2WxmzN7HfnPtMx2Vi+9xaQ4L65Hj2Vf7JK6PiXJB9Lco/M/smYxXLUVJ67StuFSW5OclhV7bOTYz68og/r42tTeceyOuduHE+ayk8vq3P+FlBVHZzkjUne3t0XbqfrnTkXzt/G2mf6XMKrq+oXq+rRa1yv7ntvAQnum+ugqbx8jfYrpvLATZgLu2bNc9fddyS5MsneSQ7YyTFXZ7ZC9eCqusf6TnXPVFV7J3nW9HT5Lw3nbkFV1QlVdVJVvbWqLkry65mF9jcu6+b8LZjpe+33Mrs07dU76L5L52L6F+fvzOx666tXeT2/J++6B2Z2/t6Q2bXu5yW5oqqOWNHP994C2nveE9jDbJnKbWu0L9Xfe+Onwi66M+duZ8bsN/W7+a5MjiSzsPf9Sc7p7j9eVu/cLa4TMvtg8ZJzk/xsd//zsjrnb/G8LskPJ/mJ7r5lB3139Vz4Pbmx3pPkoiT/b5J/ySx0vyjJ85J8uKoe2d1/PfX1vbeArLgDw6uql2S2A8VnkzxzztNhJ3X3A7u7MlsB/OnMQsQnq+qQ+c6MtVTVj2W2yv7m7v74vOfDrunuX50+J/SP3X1zd/9tdz8/sw0y7p7kpPnOkB0R3DfX0l+gW9ZoX6q/fuOnwi66M+duZ8estTLBTpi2Hnt7ZlsLPrq7r1vRxblbcFOIeH9mH96/X5LfXdbs/C2I6RKZ383sMojX7uSwXT0Xfk/Ox9IH+x+1rM733gIS3DfX56ZyrWvzlnZSWOsaeOZnzXM3/TJ7SGYfiPziTo55UGb/XHhVd/vnwjupql6a2X7Bf5tZaF/tBiLO3SC6+0uZ/QH2fVX17VO187c47pnZ1/TgJLcuv3lPZjujJcmpU93bpue7dC66+6YkX05yz6l9Jb8nN8bS5WnLd7XzvbeABPfNdf5UPrZW3N2xqr4tyeGZXfP1Pzd7YuzQeVN59Cptj8psN6CLu/u2nRzzuBV92EVV9ctJ3prkU5mF9n9ao6tzN5Z/M5Vfn0rnb3HcluR31nh8curz0en50mU0d+ZcOH+bb2k3u+Uh3PfeIpr3RvJ72iNuwLSQj+zcDZj+Obt2I4qHxI0oNup8vXb6+n0iyX130Ne5W6BHZitxW1ap/5Z88wZMH3P+xnpkdm30ajdg2uVzETdg2qhzdHCS/Vap3z+z3Xo6yauX1fveW8BHTV9QNsl0E6aLkzwgyQeTXJbkxzLb4/3yJId197Xzm+Geo6qOSXLM9PSBSf59ZqsNF0111/SyWzNP/c/K7IfSGZnd+vnJmW79nOQ/9IpvqKp6cZJ3xK2f101VHZ/ktMxWZN+Z1a+V3Nrdpy0bc0ycu4UwXd70f2W2MntlZl/f70hyRGYfTv1Kksd092eWjTkmzt9Cq6qTMrtc5rnd/dsr2nb5XFTVm5O8PMlVmZ3juyV5emafgXhxd5+yYW9mNzWdo1dktgf7lzLbVeZ7kjwhszB+TpKf6u7bl405Jr73Fsu8/3LYEx9JviuzLZmuzux/6C9ltpfqfeY9tz3pkW+uEK312LrKmMMz++H21SS3JPmbJC9Lstd2jvOkJH+e2Q/Jm5L8VZLj5/3+R33sxHnrJBc4d4v5yGzLzlMyu8Tpmsyukd02fW1Pyhr/guL8LfYja6y435VzkeRnp343TeP+PMkT5/1eR31k9sfxH2S2+9b1md2w7p+TfCSze2DUGuN87y3Qw4o7AAAMwIdTAQBgAII7AAAMQHAHAIABCO4AADAAwR0AAAYguAMAwAAEdwAAGIDgDgAAAxDcAQBgAII7AAAMQHAHAIABCO4AADAAwR0AAAYguAMAwAAEdwAAGIDgDgAAAxDcAQBgAP8f6+UZfs9tx/gAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "image/png": {
       "width": 375,
       "height": 248
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#computing frequency of each note\n",
    "freq = dict(Counter(notes_))\n",
    "\n",
    "#consider only the frequencies\n",
    "no=[count for _,count in freq.items()]\n",
    "\n",
    "#plot\n",
    "plt.hist(no)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ffce20",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
    "print(len(frequent_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the training dataset with 32 timesteps of music notes\n",
    "- This will create a training dataset of size (nb notes x timestep of 32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assign a unique integer to all the notes and chords instead of the music notation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "unique_notes = np.unique(notes_)\n",
    "note_to_int = dict((note_, number) for number, note_ in enumerate(unique_notes))\n",
    "\n",
    "notes_array_int = []\n",
    "for i in notes_array:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append(note_to_int[j])\n",
    "    notes_array_int.append(np.array(temp))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the x and y datasets based on the timestep size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "338432f7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "no_of_timesteps = 32\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for note_ in notes_array_int:\n",
    "    for i in range(0, len(note_) - no_of_timesteps):\n",
    "        \n",
    "        #preparing input and output sequences\n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output)\n",
    "        \n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the training and test set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f154f93",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a Wavenet Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the research article, we can adapt the wavenet model with our notes/chords data to generate music.  Original paper used for the following images: [link](https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio)\n",
    "\n",
    "\n",
    "\n",
    "We start by building the causal dilated convolutianal layers like the following:\n",
    "\n",
    "![image](dilated_causal_convolutianal_layers.png)\n",
    "\n",
    "The full architecture of the wavenet model is similar to this except the residual and skip connections are not used.:\n",
    "\n",
    "![image](wavenet_architecture.png)\n",
    "\n",
    "The first model developed is essentially just the dilated causal convolutional layer.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "257f5576",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 32, 100)           22500     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 32, 64)            19264     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16, 128)           24704     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 256)            98560     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 225)               57825     \n",
      "=================================================================\n",
      "Total params: 288,645\n",
      "Trainable params: 288,645\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow Keras Libraries\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# ---- MODEL CREATION -----\n",
    "model = Sequential()\n",
    "    \n",
    "# The embedding layer creates a dense vector of size (batch, no_of_timesteps, embedding_size)\n",
    "# Weights of the vector are trained during the training process.\n",
    "# The embedding layer allows to learn relationship between notes and chords and is used normally in the natural language processing domain.\n",
    "embedding_size = 100\n",
    "model.add(Embedding(input_dim=len(unique_notes), output_dim=embedding_size, input_length=no_of_timesteps,trainable=True, embeddings_initializer='uniform'))\n",
    "\n",
    "# Creating the causal dilated convolutional layers\n",
    "# First layer\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='causal', dilation_rate=1, activation='relu', ))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(MaxPool1D(pool_size=2, padding='valid'))\n",
    "\n",
    "# Second layer\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu',dilation_rate=2,padding='causal'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(MaxPool1D(pool_size=2, padding='valid'))\n",
    "\n",
    "# Third layer\n",
    "model.add(Conv1D(filters=256, kernel_size=3, activation='relu',dilation_rate=4,padding='causal'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(MaxPool1D(pool_size=2, padding='valid'))\n",
    "          \n",
    "# Downsample the input by taking hte maximum value\n",
    "model.add(GlobalMaxPool1D())\n",
    "    \n",
    "model.add(Dense(256, activation='relu', ))\n",
    "model.add(Dense(len(unique_notes), activation='softmax'))\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24fbf067",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a callback to retain only the best model\n",
    "mc=ModelCheckpoint('best_model_depth1', monitor='val_loss', mode='min', save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b82ace3b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.6146\n",
      "Epoch 00001: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 3s 28ms/step - loss: 4.6132 - val_loss: 4.5287\n",
      "Epoch 2/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.3961\n",
      "Epoch 00002: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 25ms/step - loss: 4.3928 - val_loss: 4.3450\n",
      "Epoch 3/20\n",
      "92/93 [============================>.] - ETA: 0s - loss: 4.1265\n",
      "Epoch 00003: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 3s 28ms/step - loss: 4.1251 - val_loss: 4.1790\n",
      "Epoch 4/20\n",
      "92/93 [============================>.] - ETA: 0s - loss: 3.8951\n",
      "Epoch 00004: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 3.8965 - val_loss: 3.9989\n",
      "Epoch 5/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.7427\n",
      "Epoch 00005: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 25ms/step - loss: 3.7417 - val_loss: 3.8598\n",
      "Epoch 6/20\n",
      "92/93 [============================>.] - ETA: 0s - loss: 3.6140\n",
      "Epoch 00006: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 3s 27ms/step - loss: 3.6135 - val_loss: 3.7685\n",
      "Epoch 7/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.5079\n",
      "Epoch 00007: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 3s 28ms/step - loss: 3.5076 - val_loss: 3.7583\n",
      "Epoch 8/20\n",
      "93/93 [==============================] - ETA: 0s - loss: 3.4373\n",
      "Epoch 00008: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 3.4373 - val_loss: 3.6493\n",
      "Epoch 9/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.3440\n",
      "Epoch 00009: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 25ms/step - loss: 3.3483 - val_loss: 3.6159\n",
      "Epoch 10/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.2751\n",
      "Epoch 00010: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 3s 27ms/step - loss: 3.2739 - val_loss: 3.5671\n",
      "Epoch 11/20\n",
      "93/93 [==============================] - ETA: 0s - loss: 3.2202\n",
      "Epoch 00011: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 3.2202 - val_loss: 3.5352\n",
      "Epoch 12/20\n",
      "92/93 [============================>.] - ETA: 0s - loss: 3.1433\n",
      "Epoch 00012: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 25ms/step - loss: 3.1423 - val_loss: 3.5190\n",
      "Epoch 13/20\n",
      "92/93 [============================>.] - ETA: 0s - loss: 3.0872\n",
      "Epoch 00013: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 27ms/step - loss: 3.0866 - val_loss: 3.4648\n",
      "Epoch 14/20\n",
      "93/93 [==============================] - ETA: 0s - loss: 3.0331\n",
      "Epoch 00014: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 3.0331 - val_loss: 3.4621\n",
      "Epoch 15/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 2.9615\n",
      "Epoch 00015: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 25ms/step - loss: 2.9623 - val_loss: 3.4066\n",
      "Epoch 16/20\n",
      "93/93 [==============================] - ETA: 0s - loss: 2.9313\n",
      "Epoch 00016: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 2.9313 - val_loss: 3.3976\n",
      "Epoch 17/20\n",
      "92/93 [============================>.] - ETA: 0s - loss: 2.8708\n",
      "Epoch 00017: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 2.8713 - val_loss: 3.3713\n",
      "Epoch 18/20\n",
      "92/93 [============================>.] - ETA: 0s - loss: 2.8174\n",
      "Epoch 00018: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 2.8187 - val_loss: 3.3395\n",
      "Epoch 19/20\n",
      "93/93 [==============================] - ETA: 0s - loss: 2.7719\n",
      "Epoch 00019: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 2.7719 - val_loss: 3.3126\n",
      "Epoch 20/20\n",
      "93/93 [==============================] - ETA: 0s - loss: 2.7191\n",
      "Epoch 00020: val_loss did not improve from 3.01595\n",
      "93/93 [==============================] - 2s 26ms/step - loss: 2.7191 - val_loss: 3.3245\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_tr, y_tr,batch_size=128,epochs=20, validation_data=(x_val,y_val),verbose=1, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Increase depth of convolution network\n",
    "- Other articles show that more depth in the convolution layers and starting with smaller filter size helps to increase the quality of the music generation. See this [article](https://towardsdatascience.com/generating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 32, 100)           22500     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 32, 16)            4816      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16, 32)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 64)             6208      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 4, 128)            24704     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 2, 256)            98560     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 225)               57825     \n",
      "=================================================================\n",
      "Total params: 281,973\n",
      "Trainable params: 281,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# ---- MODEL CREATION -----\n",
    "model2 = Sequential()\n",
    "\n",
    "# The embedding layer creates a dense vector of size (batch, no_of_timesteps, embedding_size)\n",
    "# Weights of the vector are trained during the training process.\n",
    "# The embedding layer allows to learn relationship between notes and chords and is used normally in the natural language processing domain.\n",
    "embedding_size = 100\n",
    "model2.add(Embedding(input_dim=len(unique_notes), output_dim=embedding_size, input_length=no_of_timesteps,trainable=True, embeddings_initializer='uniform'))\n",
    "\n",
    "# Creating the causal dilated convolutional layers\n",
    "# ------ ADDED LAYERS --------\n",
    "# LAYER 0.1\n",
    "model2.add(Conv1D(filters=16, kernel_size=3, padding='causal', dilation_rate=1, activation='relu', ))\n",
    "model2.add(Dropout(rate=0.1))\n",
    "model2.add(MaxPool1D(pool_size=2, padding='valid'))\n",
    "\n",
    "# 0.2\n",
    "model2.add(Conv1D(filters=32, kernel_size=3, padding='causal', dilation_rate=1, activation='relu', ))\n",
    "model2.add(Dropout(rate=0.1))\n",
    "model2.add(MaxPool1D(pool_size=2, padding='valid'))\n",
    "\n",
    "# ------------------------------------\n",
    "# First layer\n",
    "model2.add(Conv1D(filters=64, kernel_size=3, padding='causal', dilation_rate=1, activation='relu', ))\n",
    "model2.add(Dropout(rate=0.2))\n",
    "model2.add(MaxPool1D(pool_size=2, padding='valid'))\n",
    "\n",
    "# Second layer\n",
    "model2.add(Conv1D(filters=128, kernel_size=3, activation='relu',dilation_rate=2,padding='causal'))\n",
    "model2.add(Dropout(rate=0.2))\n",
    "model2.add(MaxPool1D(pool_size=2, padding='valid'))\n",
    "\n",
    "# Third layer\n",
    "model2.add(Conv1D(filters=256, kernel_size=3, activation='relu',dilation_rate=4,padding='causal'))\n",
    "model2.add(Dropout(rate=0.2))\n",
    "model2.add(MaxPool1D(pool_size=2, padding='valid'))\n",
    "\n",
    "# Downsample the input by taking hte maximum value\n",
    "model2.add(GlobalMaxPool1D())\n",
    "\n",
    "model2.add(Dense(256, activation='relu', ))\n",
    "model2.add(Dense(len(unique_notes), activation='softmax'))\n",
    "\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model2.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.6444\n",
      "Epoch 00001: val_loss improved from inf to 4.52399, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 41ms/step - loss: 4.6410 - val_loss: 4.5240\n",
      "Epoch 2/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.4403\n",
      "Epoch 00002: val_loss improved from 4.52399 to 4.52077, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 42ms/step - loss: 4.4390 - val_loss: 4.5208\n",
      "Epoch 3/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.4040\n",
      "Epoch 00003: val_loss did not improve from 4.52077\n",
      "93/93 [==============================] - 2s 22ms/step - loss: 4.4064 - val_loss: 4.5435\n",
      "Epoch 4/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.3563\n",
      "Epoch 00004: val_loss improved from 4.52077 to 4.44465, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 39ms/step - loss: 4.3567 - val_loss: 4.4447\n",
      "Epoch 5/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.2783\n",
      "Epoch 00005: val_loss improved from 4.44465 to 4.35053, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 41ms/step - loss: 4.2765 - val_loss: 4.3505\n",
      "Epoch 6/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.1586\n",
      "Epoch 00006: val_loss improved from 4.35053 to 4.28370, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 39ms/step - loss: 4.1577 - val_loss: 4.2837\n",
      "Epoch 7/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 4.0287\n",
      "Epoch 00007: val_loss improved from 4.28370 to 4.14210, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 40ms/step - loss: 4.0291 - val_loss: 4.1421\n",
      "Epoch 8/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.9294\n",
      "Epoch 00008: val_loss improved from 4.14210 to 4.10737, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 44ms/step - loss: 3.9263 - val_loss: 4.1074\n",
      "Epoch 9/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.8457\n",
      "Epoch 00009: val_loss improved from 4.10737 to 4.09844, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 40ms/step - loss: 3.8479 - val_loss: 4.0984\n",
      "Epoch 10/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.7752\n",
      "Epoch 00010: val_loss improved from 4.09844 to 3.95447, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 40ms/step - loss: 3.7762 - val_loss: 3.9545\n",
      "Epoch 11/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.7009\n",
      "Epoch 00011: val_loss improved from 3.95447 to 3.94172, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 40ms/step - loss: 3.7024 - val_loss: 3.9417\n",
      "Epoch 12/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.6600\n",
      "Epoch 00012: val_loss improved from 3.94172 to 3.88930, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 39ms/step - loss: 3.6621 - val_loss: 3.8893\n",
      "Epoch 13/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.6035\n",
      "Epoch 00013: val_loss improved from 3.88930 to 3.88607, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 39ms/step - loss: 3.6045 - val_loss: 3.8861\n",
      "Epoch 14/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.5596\n",
      "Epoch 00014: val_loss improved from 3.88607 to 3.81640, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 45ms/step - loss: 3.5615 - val_loss: 3.8164\n",
      "Epoch 15/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.5048\n",
      "Epoch 00015: val_loss improved from 3.81640 to 3.78874, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 39ms/step - loss: 3.5050 - val_loss: 3.7887\n",
      "Epoch 16/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.4733\n",
      "Epoch 00016: val_loss improved from 3.78874 to 3.74362, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 40ms/step - loss: 3.4727 - val_loss: 3.7436\n",
      "Epoch 17/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.4480\n",
      "Epoch 00017: val_loss improved from 3.74362 to 3.69312, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 39ms/step - loss: 3.4470 - val_loss: 3.6931\n",
      "Epoch 18/20\n",
      "93/93 [==============================] - ETA: 0s - loss: 3.4191\n",
      "Epoch 00018: val_loss did not improve from 3.69312\n",
      "93/93 [==============================] - 2s 23ms/step - loss: 3.4191 - val_loss: 3.7156\n",
      "Epoch 19/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.3805\n",
      "Epoch 00019: val_loss did not improve from 3.69312\n",
      "93/93 [==============================] - 2s 22ms/step - loss: 3.3812 - val_loss: 3.7207\n",
      "Epoch 20/20\n",
      "91/93 [============================>.] - ETA: 0s - loss: 3.3585\n",
      "Epoch 00020: val_loss improved from 3.69312 to 3.67377, saving model to best_model_depth2\n",
      "INFO:tensorflow:Assets written to: best_model_depth2\\assets\n",
      "93/93 [==============================] - 4s 39ms/step - loss: 3.3562 - val_loss: 3.6738\n"
     ]
    }
   ],
   "source": [
    "# Run the model and save the best model\n",
    "mc = ModelCheckpoint('best_model_depth2', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "history = model2.fit(x_tr, y_tr, batch_size=128, epochs=20, validation_data=(x_val, y_val), verbose=1, callbacks=[mc])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM Prediction\n",
    "- Instead of using convolution layers, let us see how it works with LSTM layers\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 32, 100)           22500     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 225)               57825     \n",
      "=================================================================\n",
      "Total params: 230,597\n",
      "Trainable params: 230,597\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Simple LSTM model\n",
    "K.clear_session()\n",
    "\n",
    "# ---- MODEL CREATION -----\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(input_dim=len(unique_notes), output_dim=embedding_size, input_length=no_of_timesteps,trainable=True, embeddings_initializer='uniform'))\n",
    "model3.add(LSTM(units=128, return_sequences=False))\n",
    "model3.add(Dense(units=256, activation='relu'))\n",
    "model3.add(Dense(units=len(unique_notes), activation='softmax'))\n",
    "model3.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model3.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run the model and save the best model\n",
    "mc = ModelCheckpoint('best_model_lstm', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "history = model3.fit(x_tr, y_tr, batch_size=128, epochs=20, validation_data=(x_val, y_val), verbose=1, callbacks=[mc])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make predictions using the best model obtained"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the best model to create the upcoming musics.  (best model for the 3 different approaches shown)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "b577243c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loading\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('best_model_depth2')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make a music, we use a random segment of a music from the validation dataset and then the model will predict the next notes over and over again."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple prediction\n",
    "- Using a segment of size 32, we predict the next 32 notes/chords of the music by using the highest predicted note"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "412df988",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115 213 187 115 213 187 115 203 187 115 193 187 111 188 187 111 178 187\n",
      " 111 193 187 111 188 186  97 188 187  97 178 187 111 193]\n",
      "[187, 111, 193, 187, 111, 193, 187, 111, 178, 187, 111, 188, 187, 111, 188, 187, 111, 193, 187, 111, 193, 187, 111, 189, 187, 111, 193, 188, 111, 189, 188, 111]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "ind = np.random.randint(0,len(x_val)-1)\n",
    "initial_music = x_val[ind]  # will not change in the code\n",
    "random_music = x_val[ind]  # will change during the creation of the music (first notes removed)\n",
    "\n",
    "predictions=[]\n",
    "for i in range(32):  # we will predict the second half of the song.\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    y_pred= np.argmax(prob,axis=0)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "\n",
    "print(initial_music)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform back into notes:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [],
   "source": [
    "int_to_note = dict((number, note_) for number, note_ in enumerate(unique_notes))\n",
    "def create_music(note_int):\n",
    "    notes = [int_to_note[i] for i in note_int]\n",
    "    return notes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "88712601",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the predicted notes with the function\n",
    "predicted_notes = create_music(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert notes into MIDI file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2f1fb29a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_midi(prediction_output, name):\n",
    "   \n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        \n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                \n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        # pattern is a note\n",
    "        else:\n",
    "            \n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 1\n",
    "    midi_stream = stream.Stream(output_notes, tempo=120)\n",
    "    midi_stream.write('midi', fp=name+'.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3e38af04",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "convert_to_midi(predicted_notes, name='base_32_2')\n",
    "convert_to_midi(create_music(initial_music)+predicted_notes, name=\"base_64_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make a random decision out of the best notes\n",
    "- Similar to in Reinforcement Learning, we want to perform some exploration to allow for more original creation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115 213 187 115 213 187 115 203 187 115 193 187 111 188 187 111 178 187\n",
      " 111 193 187 111 188 186  97 188 187  97 178 187 111 193]\n",
      "[187, 111, 193, 187, 111, 193, 187, 194, 178, 187, 111, 188, 187, 16, 188, 187, 193, 218, 187, 111, 178, 187, 72, 178, 187, 111, 14, 178, 111, 188, 211, 111]\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters:\n",
    "epsilon = 0.2\n",
    "top_n = 20\n",
    "\n",
    "random_music = initial_music\n",
    "predictions=[]\n",
    "for i in range(32):  # we will predict the second half of the song.\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    # Check the probabilities\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    prob_n = np.argsort(prob)[-top_n:]  # top n notes\n",
    "\n",
    "    # Select the note\n",
    "    rand = np.random.random()\n",
    "    if rand>epsilon:\n",
    "        # Pick best note\n",
    "        y_pred= np.argmax(prob,axis=0)\n",
    "    else:\n",
    "        # Pick random note\n",
    "        y_pred = prob_n[np.random.randint(0, len(prob_n))]\n",
    "\n",
    "    # Prediction\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    # Change the random music size and length with the new note\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "\n",
    "print(initial_music)\n",
    "print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [],
   "source": [
    "predicted_notes_e = create_music(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "# Export\n",
    "convert_to_midi(predicted_notes_e, name='epsilon_32_2')\n",
    "convert_to_midi(create_music(initial_music)+predicted_notes_e, name=\"epsilon_64_2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Beam Search\n",
    "- We'll look two notes away and see what is the highest prediction and based on that, select the current note."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115 213 187 115 213 187 115 203 187 115 193 187 111 188 187 111 178 187\n",
      " 111 193 187 111 188 186  97 188 187  97 178 187 111 193]\n",
      "[187, 111, 193, 187, 97, 193, 187, 111, 178, 187, 111, 188, 187, 111, 188, 187, 111, 193, 187, 111, 193, 187, 111, 189, 187, 111, 193, 188, 111, 189, 188, 111]\n"
     ]
    }
   ],
   "source": [
    "# Perform a beam search with looking two notes ahead.\n",
    "\n",
    "level_1 = 5  # beam search first level limited scope\n",
    "predictions=[]\n",
    "random_music = initial_music\n",
    "for i in range(32):  # we will predict the second half of the song.\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    # Check the probabilities\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    prob_n_notes = np.argsort(prob)[-level_1:]  # the notes with the highest probabilities.\n",
    "    prob_n = np.sort(prob)[-level_1:]  # the probabilities\n",
    "\n",
    "    # Perform the beam search: Check the next predictions to check what is the highest probability path.\n",
    "    max_prob_path = {}  # key is the following y_pred and value is the probability\n",
    "    for prob, note_ in zip(prob_n, prob_n_notes):\n",
    "        # Create the new temporary music path\n",
    "        random_music_temp = np.insert(random_music[0], len(random_music[0]), note_)\n",
    "        random_music_temp = random_music_temp[1:].reshape((1,-1))\n",
    "\n",
    "        # Re-run the prediction after this note selection\n",
    "        prob2 = model.predict(random_music_temp)[0]\n",
    "\n",
    "        # Select the top\n",
    "        prob2_max = np.max(prob2)\n",
    "\n",
    "        # Joint probability\n",
    "        joint = prob * prob2_max\n",
    "\n",
    "        # Append in dictionary\n",
    "        max_prob_path[note_] = joint\n",
    "\n",
    "    # Select the note from the maximum joint path\n",
    "    y_pred = max(max_prob_path, key=max_prob_path.get)\n",
    "\n",
    "    # Actual Prediction\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    # Change the random music size and length with the new note\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "\n",
    "print(initial_music)\n",
    "print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [],
   "source": [
    "predicted_notes_b = create_music(predictions)\n",
    "# Export\n",
    "convert_to_midi(predicted_notes_b, name='beam_32_2')\n",
    "convert_to_midi(create_music(initial_music) + predicted_notes_b, name=\"beam_64_2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Very similar to the initial simple predictions meaning that the first highest prediction is usually the best path! This is pretty intuitive but if the beam search went further it might change the outcome which would be interesting to see."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "If we are a musician trying to discover new music style.  We could use different methods to change the way the music is created.  If we feed our initial composition, then we can explore with different hyper parameters, different suggested music that would help the creation process.  This is why we have shown three different methods to generate music after the model predictions.\n",
    "\n",
    "From the model predictions, we have seen that by adding depth in the convolution layers starting with smaller filter size really helped to have better music which represents beethoven more.  Granted, we are only comparing with 1 starting segment and we are no musical experts...\n",
    "\n",
    "### Limitations\n",
    "In our case, the sampling of the music was done note by note or chord by chord.  Ideally, we would want to develop a sampling on a time basis and have notes or no notes so the tempo would vary.  Also, the midi format has sound intensity which and other parameters which are removed.  Only the notes or chords from the piano channel is extracted so this is a very primitive model compared to other music generator such as MuseNet and Jukebox by OpenAI.\n",
    "\n",
    "### Future idea...\n",
    "Also, the way the creation works is by evaluating the accuracy of the model to predict the next note accurately.  But it would be interesting to analyse a way to generate pleasing music and to evaluate that outcome.  Difficult to say with no music background."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}